{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (3.10.1)\n",
      "Collecting underthesea\n",
      "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: gensim in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: streamlit in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (1.42.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from datasets) (1.4.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from underthesea) (8.1.8)\n",
      "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
      "  Downloading python_crfsuite-0.9.11-cp312-cp312-win_amd64.whl.metadata (4.4 kB)\n",
      "Collecting nltk (from underthesea)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from underthesea) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pc\\anaconda3\\envs\\aka_env\\lib\\site-packages (from underthesea) (1.6.1)\n",
      "Collecting underthesea-core==1.0.4 (from underthesea)\n",
      "  Downloading underthesea_core-1.0.4.tar.gz (560 kB)\n",
      "     ---------------------------------------- 0.0/560.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/560.4 kB ? eta -:--:--\n",
      "     ------------------ --------------------- 262.1/560.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 560.4/560.4 kB 1.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 106, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 97, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 386, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 95, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 546, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 427, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_updated_criteria(candidate)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 230, in _get_updated_criteria\n",
      "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 156, in __bool__\n",
      "    return bool(self._sequence)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 174, in __bool__\n",
      "    return any(self)\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 162, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "                       ^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 53, in _iter_built\n",
      "    candidate = func()\n",
      "                ^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 187, in _make_candidate_from_link\n",
      "    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 233, in _make_base_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "                                       ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 304, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 159, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 236, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 315, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 527, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 642, in _prepare_linked_requirement\n",
      "    dist = _get_prepared_distribution(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 72, in _get_prepared_distribution\n",
      "    abstract_dist.prepare_distribution_metadata(\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 56, in prepare_distribution_metadata\n",
      "    self._install_build_reqs(finder)\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 126, in _install_build_reqs\n",
      "    build_reqs = self._get_build_requires_wheel()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 103, in _get_build_requires_wheel\n",
      "    return backend.get_requires_for_build_wheel()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 702, in get_requires_for_build_wheel\n",
      "    return super().get_requires_for_build_wheel(config_settings=cs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 196, in get_requires_for_build_wheel\n",
      "    return self._call_hook(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pc\\anaconda3\\envs\\aka_env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 402, in _call_hook\n",
      "    raise BackendUnavailable(\n",
      "pip._vendor.pyproject_hooks._impl.BackendUnavailable: Cannot import 'maturin'\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets matplotlib underthesea gensim streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import joblib\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G·ªôp data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, output_dir=\"./processed_data/\"):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Mapping chu·∫©n cho sentiment\n",
    "        self.sentiment_mapping = {\n",
    "            \"negative\": 0, \n",
    "            \"neutral\": 1, \n",
    "            \"positive\": 2,\n",
    "            # Mapping ng∆∞·ª£c\n",
    "            0: 0, 1: 1, 2: 2,\n",
    "            # Bi·∫øn th·ªÉ\n",
    "            \"Negative\": 0, \"Neutral\": 1, \"Positive\": 2,\n",
    "            \"NEGATIVE\": 0, \"NEUTRAL\": 1, \"POSITIVE\": 2\n",
    "        }\n",
    "\n",
    "    def normalize_sentiment(self, value):\n",
    "        if pd.isna(value):\n",
    "            return None\n",
    "            \n",
    "        if isinstance(value, (int, float)):\n",
    "            return int(value) if value in [0, 1, 2] else None\n",
    "            \n",
    "        if isinstance(value, str):\n",
    "            value = value.lower().strip()\n",
    "            return self.sentiment_mapping.get(value)\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def validate_data(self, df, name=\"\"):\n",
    "        logging.info(f\"\\nValidating {name}:\")\n",
    "        logging.info(f\"Total samples: {len(df)}\")\n",
    "        logging.info(f\"Missing values: {df.isnull().sum().to_dict()}\")\n",
    "        \n",
    "        sentiment_dist = df['sentiment'].value_counts()\n",
    "        logging.info(\"\\nSentiment distribution:\")\n",
    "        for sent, count in sentiment_dist.items():\n",
    "            logging.info(f\"Sentiment {sent}: {count} ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "    def load_and_process_data(self, is_synthetic=False, **paths):\n",
    "        try:\n",
    "            if is_synthetic:\n",
    "                df = pd.read_csv(paths['path'])\n",
    "                df = df[['sentence', 'sentiment', 'topic']].copy()\n",
    "            else:\n",
    "                ds = load_dataset(\"uitnlp/vietnamese_students_feedback\")\n",
    "                df = ds[paths['split']].to_pandas()\n",
    "                df = df.rename(columns={'text': 'sentence', 'label': 'sentiment'})\n",
    "                df['topic'] = 'general'\n",
    "\n",
    "            # Chu·∫©n h√≥a sentiment\n",
    "            df['sentiment'] = df['sentiment'].apply(self.normalize_sentiment)\n",
    "            df = df.dropna(subset=['sentiment'])\n",
    "            df['sentiment'] = df['sentiment'].astype(int)\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing data: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def merge_and_save_data(self):\n",
    "        \"\"\"X·ª≠ l√Ω v√† g·ªôp d·ªØ li·ªáu\"\"\"\n",
    "        dfs = {\n",
    "            'train': self.load_and_process_data(split='train'),\n",
    "            'val': self.load_and_process_data(split='validation'),\n",
    "            'test': self.load_and_process_data(split='test')\n",
    "        }\n",
    "        \n",
    "        # Load d·ªØ li·ªáu synthetic\n",
    "        synthetic = {\n",
    "            'train': self.load_and_process_data(\n",
    "                is_synthetic=True, \n",
    "                path=\"./chatgpt_data/synthetic_train.csv\"\n",
    "            ),\n",
    "            'val': self.load_and_process_data(\n",
    "                is_synthetic=True,\n",
    "                path=\"./chatgpt_data/synthetic_val.csv\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        if any(df is None for df in dfs.values()) or any(df is None for df in synthetic.values()):\n",
    "            return\n",
    "\n",
    "        # G·ªôp v√† x·ª≠ l√Ω\n",
    "        combined = {\n",
    "            'train': pd.concat([dfs['train'], synthetic['train']], ignore_index=True),\n",
    "            'val': pd.concat([dfs['val'], synthetic['val']], ignore_index=True),\n",
    "            'test': dfs['test']\n",
    "        }\n",
    "\n",
    "        for split in ['train', 'val']:\n",
    "            combined[split] = combined[split].drop_duplicates(subset='sentence')\n",
    "            self.validate_data(combined[split], f\"{split} set\")\n",
    "\n",
    "        for split, df in combined.items():\n",
    "            df.to_csv(f\"{self.output_dir}{split}.csv\", index=False)\n",
    "\n",
    "        self.plot_distributions(combined)\n",
    "        \n",
    "        return combined\n",
    "\n",
    "    def plot_distributions(self, dfs):\n",
    "        \"\"\"V·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        counts = {split: len(df) for split, df in dfs.items()}\n",
    "        plt.bar(counts.keys(), counts.values())\n",
    "        plt.title(\"Ph√¢n b·ªë s·ªë l∆∞·ª£ng m·∫´u\")\n",
    "        for i, (split, count) in enumerate(counts.items()):\n",
    "            plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "        plt.savefig(f\"{self.output_dir}sample_distribution.png\")\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        train_sentiments = dfs['train']['sentiment'].value_counts()\n",
    "        colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "        plt.pie(\n",
    "            train_sentiments,\n",
    "            labels=[f\"Sentiment {i}\" for i in range(3)],\n",
    "            colors=colors,\n",
    "            autopct='%1.1f%%'\n",
    "        )\n",
    "        plt.title(\"Ph√¢n b·ªë sentiment trong t·∫≠p train\")\n",
    "        plt.savefig(f\"{self.output_dir}sentiment_distribution.png\")\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = DataProcessor()\n",
    "    data = processor.merge_and_save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c m·ªõi\n",
    "new_data_dir = \"processed_data\"\n",
    "\n",
    "# ƒê·ªçc c√°c t·ªáp CSV\n",
    "new_train = pd.read_csv(os.path.join(new_data_dir, \"train.csv\"))\n",
    "new_val = pd.read_csv(os.path.join(new_data_dir, \"val.csv\"))\n",
    "new_test = pd.read_csv(os.path.join(new_data_dir, \"test.csv\"))\n",
    "# Hi·ªÉn th·ªã m·ªôt v√†i d√≤ng d·ªØ li·ªáu m·ªõi\n",
    "print(new_train.head())\n",
    "print(new_val.head())\n",
    "print(new_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"10 samples from new_train with sentiment = 0:\")\n",
    "print(new_train[new_train['sentiment'] == 0].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"10 samples from new_train with sentiment = 1:\")\n",
    "print(new_train[new_train['sentiment'] == 1].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"10 samples from new_train with sentiment = 1:\")\n",
    "print(new_train[new_train['sentiment'] == 2].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = new_train['sentiment'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Ph√¢n b·ªë c√°c nh√£n trong t·∫≠p hu·∫•n luy·ªán\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c m·ªõi\n",
    "new_data_dir = \"processed_data\"\n",
    "\n",
    "# ƒê·ªçc c√°c t·ªáp CSV\n",
    "new_train = pd.read_csv(os.path.join(new_data_dir, \"train.csv\"))\n",
    "new_val = pd.read_csv(os.path.join(new_data_dir, \"val.csv\"))\n",
    "new_test = pd.read_csv(os.path.join(new_data_dir, \"test.csv\"))\n",
    "\n",
    "# T·ª´ ƒëi·ªÉn vi·∫øt t·∫Øt c·∫ßn thay th·∫ø\n",
    "TEENCODE_DICT = {\n",
    "    'ko': 'kh√¥ng', 'k': 'kh√¥ng', 'kh': 'kh√¥ng', 'kg': 'kh√¥ng',\n",
    "    'dc': 'ƒë∆∞·ª£c', 'vs': 'v·ªõi', 'nx': 'nh·ªØng', 'tk': 't√†i kho·∫£n',\n",
    "    'r': 'r·ªìi', 'wa': 'qu√°', 'ht': 'hi·ªán t·∫°i'\n",
    "}\n",
    "\n",
    "# X·ª≠ l√Ω d·ªØ li·ªáu\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Chu·∫©n h√≥a Unicode & chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng\n",
    "    text = unicodedata.normalize('NFKC', text.lower())\n",
    "\n",
    "    # Lo·∫°i b·ªè HTML & URLs\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Lo·∫°i b·ªè emoji\n",
    "    text = re.sub(r\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"]+\", '', text, flags=re.UNICODE)\n",
    "\n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "    text = re.sub(r'[^\\s\\w√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë_]',' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Thay th·∫ø vi·∫øt t·∫Øt b·∫±ng t·ª´ ƒë·∫ßy ƒë·ªß\n",
    "    text = ' '.join([TEENCODE_DICT.get(word.lower(), word) for word in text.split()])\n",
    "\n",
    "    # T√°ch t·ª´ ti·∫øng Vi·ªát\n",
    "    text = word_tokenize(text, format=\"text\")\n",
    "\n",
    "    return text\n",
    "\n",
    "# X·ª≠ l√Ω t·ª´ng t·∫≠p d·ªØ li·ªáu v√† l∆∞u k·∫øt qu·∫£\n",
    "for name, df in [(\"train\", new_train), (\"val\", new_val), (\"test\", new_test)]:\n",
    "    print(f\"ƒêang x·ª≠ l√Ω {name}...\")\n",
    "\n",
    "    # X·ª≠ l√Ω t·∫•t c·∫£ vƒÉn b·∫£n\n",
    "    df[\"processed_text\"] = df[\"sentence\"].apply(preprocess_text)\n",
    "\n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    output_path = os.path.join(new_data_dir, f\"{name}_processed.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"‚úî ƒê√£ l∆∞u: {output_path}\")\n",
    "\n",
    "    # In v√≠ d·ª• k·∫øt qu·∫£ x·ª≠ l√Ω\n",
    "    print(\"\\nV√≠ d·ª• k·∫øt qu·∫£ x·ª≠ l√Ω:\")\n",
    "    for i in range(min(5, len(df))):\n",
    "        print(f\"\\nG·ªëc: {df['sentence'].iloc[i]}\")\n",
    "        print(f\"Sau x·ª≠ l√Ω: {df['processed_text'].iloc[i]}\")\n",
    "\n",
    "print(\"Ho√†n th√†nh x·ª≠ l√Ω t·∫•t c·∫£ t·∫≠p d·ªØ li·ªáu!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG = 3 PH∆Ø∆†NG PH√ÅP TDIDF, WORD2VEC(CBOW,SKIP-GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a c√°c t·ªáp CSV ƒë√£ x·ª≠ l√Ω\n",
    "processed_data_dir = \"processed_data\"\n",
    "\n",
    "# ƒê·ªçc c√°c t·ªáp CSV ƒë√£ x·ª≠ l√Ω\n",
    "df_train = pd.read_csv(os.path.join(processed_data_dir, \"train_processed.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(processed_data_dir, \"val_processed.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(processed_data_dir, \"test_processed.csv\"))\n",
    "\n",
    "y = df_train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models\\\\TF-IDF_vectorizer.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng TF-IDF --------------------\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_train['processed_text'])\n",
    "joblib.dump(tfidf_vectorizer, os.path.join(model_dir, \"TF-IDF_vectorizer.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng = word2vec 2 pp (CBOW & Skip-gram) --------------------\n",
    "sentences = [text.split() for text in df_train['processed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 15:05:26,436 - INFO - collecting all words and their counts\n",
      "2025-02-19 15:05:26,436 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-02-19 15:05:26,453 - INFO - PROGRESS: at sentence #10000, processed 95493 words, keeping 3820 word types\n",
      "2025-02-19 15:05:26,472 - INFO - collected 6163 word types from a corpus of 201166 raw words and 19569 sentences\n",
      "2025-02-19 15:05:26,472 - INFO - Creating a fresh vocabulary\n",
      "2025-02-19 15:05:26,485 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 3145 unique words (51.03% of original 6163, drops 3018)', 'datetime': '2025-02-19T15:05:26.485344', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-02-19 15:05:26,485 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 198148 word corpus (98.50% of original 201166, drops 3018)', 'datetime': '2025-02-19T15:05:26.485344', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-02-19 15:05:26,502 - INFO - deleting the raw counts dictionary of 6163 items\n",
      "2025-02-19 15:05:26,503 - INFO - sample=0.001 downsamples 72 most-common words\n",
      "2025-02-19 15:05:26,504 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 141037.1050610621 word corpus (71.2%% of prior 198148)', 'datetime': '2025-02-19T15:05:26.504287', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-02-19 15:05:26,532 - INFO - estimated required memory for 3145 words and 100 dimensions: 4088500 bytes\n",
      "2025-02-19 15:05:26,538 - INFO - resetting layer weights\n",
      "2025-02-19 15:05:26,541 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-02-19T15:05:26.541569', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2025-02-19 15:05:26,543 - INFO - Word2Vec lifecycle event {'msg': 'training model with 12 workers on 3145 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-02-19T15:05:26.543682', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2025-02-19 15:05:26,614 - INFO - EPOCH 0: training on 201166 raw words (140908 effective words) took 0.1s, 2320929 effective words/s\n",
      "2025-02-19 15:05:26,697 - INFO - EPOCH 1: training on 201166 raw words (141147 effective words) took 0.1s, 2578498 effective words/s\n",
      "2025-02-19 15:05:26,768 - INFO - EPOCH 2: training on 201166 raw words (141198 effective words) took 0.1s, 2407300 effective words/s\n",
      "2025-02-19 15:05:26,829 - INFO - EPOCH 3: training on 201166 raw words (141227 effective words) took 0.1s, 2508005 effective words/s\n",
      "2025-02-19 15:05:26,898 - INFO - EPOCH 4: training on 201166 raw words (141077 effective words) took 0.1s, 2658049 effective words/s\n",
      "2025-02-19 15:05:26,898 - INFO - Word2Vec lifecycle event {'msg': 'training on 1005830 raw words (705557 effective words) took 0.4s, 1925747 effective words/s', 'datetime': '2025-02-19T15:05:26.898763', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2025-02-19 15:05:26,898 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=3145, vector_size=100, alpha=0.025>', 'datetime': '2025-02-19T15:05:26.898763', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "2025-02-19 15:05:26,898 - INFO - collecting all words and their counts\n",
      "2025-02-19 15:05:26,914 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-02-19 15:05:26,919 - INFO - PROGRESS: at sentence #10000, processed 95493 words, keeping 3820 word types\n",
      "2025-02-19 15:05:26,941 - INFO - collected 6163 word types from a corpus of 201166 raw words and 19569 sentences\n",
      "2025-02-19 15:05:26,941 - INFO - Creating a fresh vocabulary\n",
      "2025-02-19 15:05:26,955 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 3145 unique words (51.03% of original 6163, drops 3018)', 'datetime': '2025-02-19T15:05:26.955959', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-02-19 15:05:26,956 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 198148 word corpus (98.50% of original 201166, drops 3018)', 'datetime': '2025-02-19T15:05:26.956958', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-02-19 15:05:26,968 - INFO - deleting the raw counts dictionary of 6163 items\n",
      "2025-02-19 15:05:26,968 - INFO - sample=0.001 downsamples 72 most-common words\n",
      "2025-02-19 15:05:26,968 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 141037.1050610621 word corpus (71.2%% of prior 198148)', 'datetime': '2025-02-19T15:05:26.968524', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-02-19 15:05:26,990 - INFO - estimated required memory for 3145 words and 100 dimensions: 4088500 bytes\n",
      "2025-02-19 15:05:26,990 - INFO - resetting layer weights\n",
      "2025-02-19 15:05:26,995 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-02-19T15:05:26.995577', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2025-02-19 15:05:26,996 - INFO - Word2Vec lifecycle event {'msg': 'training model with 12 workers on 3145 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-02-19T15:05:26.996657', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2025-02-19 15:05:27,110 - INFO - EPOCH 0: training on 201166 raw words (141185 effective words) took 0.1s, 1392615 effective words/s\n",
      "2025-02-19 15:05:27,230 - INFO - EPOCH 1: training on 201166 raw words (141008 effective words) took 0.1s, 1414039 effective words/s\n",
      "2025-02-19 15:05:27,336 - INFO - EPOCH 2: training on 201166 raw words (140829 effective words) took 0.1s, 1484808 effective words/s\n",
      "2025-02-19 15:05:27,462 - INFO - EPOCH 3: training on 201166 raw words (140802 effective words) took 0.1s, 1333068 effective words/s\n",
      "2025-02-19 15:05:27,590 - INFO - EPOCH 4: training on 201166 raw words (141097 effective words) took 0.1s, 1397817 effective words/s\n",
      "2025-02-19 15:05:27,590 - INFO - Word2Vec lifecycle event {'msg': 'training on 1005830 raw words (704921 effective words) took 0.6s, 1187402 effective words/s', 'datetime': '2025-02-19T15:05:27.590823', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2025-02-19 15:05:27,591 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=3145, vector_size=100, alpha=0.025>', 'datetime': '2025-02-19T15:05:27.591823', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec models\n",
    "w2v_cbow = Word2Vec(sentences, vector_size=100, window=5, sg=0, min_count=2, workers=multiprocessing.cpu_count())\n",
    "w2v_sg = Word2Vec(sentences, vector_size=100, window=5, sg=1, min_count=2, workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 15:51:52,682 - INFO - Word2Vec lifecycle event {'fname_or_handle': 'saved_models/word2vec_skipgram.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-02-19T15:51:52.682858', 'gensim': '4.3.3', 'python': '3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:07:43) [MSC v.1942 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "2025-02-19 15:51:52,683 - INFO - not storing attribute cum_table\n",
      "2025-02-19 15:51:52,689 - INFO - saved saved_models/word2vec_skipgram.model\n"
     ]
    }
   ],
   "source": [
    "w2v_sg.save(\"saved_models/word2vec_skipgram.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sentences to vectors using a trained Word2Vec model\n",
    "def sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Convert all sentences to vectors using CBOW and Skip-gram\n",
    "X_w2v_cbow = np.array([sentence_vector(text, w2v_cbow) for text in df_train['processed_text']])\n",
    "X_w2v_sg = np.array([sentence_vector(text, w2v_sg) for text in df_train['processed_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Define models --------------------\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, activation='relu', solver='adam'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"SVM\": SVC(kernel='linear', probability=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\aka_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Train & Evaluate Models --------------------\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "results = []\n",
    "model_dir = \"saved_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for feature_name, X in [(\"TF-IDF\", X_tfidf), (\"Word2Vec CBOW\", X_w2v_cbow), (\"Word2Vec Skip-gram\", X_w2v_sg)]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        # save results\n",
    "        results.append({\n",
    "            \"Feature\": feature_name,\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"Training Time (s)\": train_time\n",
    "        })\n",
    "        model_filename = f\"{model_dir}/{feature_name}_{model_name}.pkl\"\n",
    "        joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Training Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.834440</td>\n",
       "      <td>0.824385</td>\n",
       "      <td>0.824309</td>\n",
       "      <td>0.834440</td>\n",
       "      <td>0.399071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>MLP Classifier</td>\n",
       "      <td>0.817833</td>\n",
       "      <td>0.817254</td>\n",
       "      <td>0.816707</td>\n",
       "      <td>0.817833</td>\n",
       "      <td>311.664151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.833163</td>\n",
       "      <td>0.818536</td>\n",
       "      <td>0.830403</td>\n",
       "      <td>0.833163</td>\n",
       "      <td>17.189322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.836995</td>\n",
       "      <td>0.825952</td>\n",
       "      <td>0.826777</td>\n",
       "      <td>0.836995</td>\n",
       "      <td>51.411999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word2Vec CBOW</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.781809</td>\n",
       "      <td>0.757564</td>\n",
       "      <td>0.771994</td>\n",
       "      <td>0.781809</td>\n",
       "      <td>0.665790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Word2Vec CBOW</td>\n",
       "      <td>MLP Classifier</td>\n",
       "      <td>0.811191</td>\n",
       "      <td>0.801486</td>\n",
       "      <td>0.800449</td>\n",
       "      <td>0.811191</td>\n",
       "      <td>20.208349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Word2Vec CBOW</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.780020</td>\n",
       "      <td>0.763212</td>\n",
       "      <td>0.773511</td>\n",
       "      <td>0.780020</td>\n",
       "      <td>15.893234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Word2Vec CBOW</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.768779</td>\n",
       "      <td>0.720087</td>\n",
       "      <td>0.797292</td>\n",
       "      <td>0.768779</td>\n",
       "      <td>48.182016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Word2Vec Skip-gram</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.800204</td>\n",
       "      <td>0.779410</td>\n",
       "      <td>0.787791</td>\n",
       "      <td>0.800204</td>\n",
       "      <td>0.458314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Word2Vec Skip-gram</td>\n",
       "      <td>MLP Classifier</td>\n",
       "      <td>0.833418</td>\n",
       "      <td>0.825379</td>\n",
       "      <td>0.825073</td>\n",
       "      <td>0.833418</td>\n",
       "      <td>17.331103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Word2Vec Skip-gram</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.806847</td>\n",
       "      <td>0.785765</td>\n",
       "      <td>0.802059</td>\n",
       "      <td>0.806847</td>\n",
       "      <td>16.459986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Word2Vec Skip-gram</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.801737</td>\n",
       "      <td>0.770053</td>\n",
       "      <td>0.802205</td>\n",
       "      <td>0.801737</td>\n",
       "      <td>39.536366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Feature                Model  Accuracy  F1 Score  Precision  \\\n",
       "0               TF-IDF  Logistic Regression  0.834440  0.824385   0.824309   \n",
       "1               TF-IDF       MLP Classifier  0.817833  0.817254   0.816707   \n",
       "2               TF-IDF        Random Forest  0.833163  0.818536   0.830403   \n",
       "3               TF-IDF                  SVM  0.836995  0.825952   0.826777   \n",
       "4        Word2Vec CBOW  Logistic Regression  0.781809  0.757564   0.771994   \n",
       "5        Word2Vec CBOW       MLP Classifier  0.811191  0.801486   0.800449   \n",
       "6        Word2Vec CBOW        Random Forest  0.780020  0.763212   0.773511   \n",
       "7        Word2Vec CBOW                  SVM  0.768779  0.720087   0.797292   \n",
       "8   Word2Vec Skip-gram  Logistic Regression  0.800204  0.779410   0.787791   \n",
       "9   Word2Vec Skip-gram       MLP Classifier  0.833418  0.825379   0.825073   \n",
       "10  Word2Vec Skip-gram        Random Forest  0.806847  0.785765   0.802059   \n",
       "11  Word2Vec Skip-gram                  SVM  0.801737  0.770053   0.802205   \n",
       "\n",
       "      Recall  Training Time (s)  \n",
       "0   0.834440           0.399071  \n",
       "1   0.817833         311.664151  \n",
       "2   0.833163          17.189322  \n",
       "3   0.836995          51.411999  \n",
       "4   0.781809           0.665790  \n",
       "5   0.811191          20.208349  \n",
       "6   0.780020          15.893234  \n",
       "7   0.768779          48.182016  \n",
       "8   0.800204           0.458314  \n",
       "9   0.833418          17.331103  \n",
       "10  0.806847          16.459986  \n",
       "11  0.801737          39.536366  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ K·∫øt qu·∫£ d·ª± ƒëo√°n:\n",
      "{'TF-IDF + Logistic Regression': 't√≠ch c·ª±c', 'Word2Vec Skip-gram + MLP': 't√≠ch c·ª±c'}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n ƒë·∫øn m√¥ h√¨nh ƒë√£ l∆∞u\n",
    "model_dir = \"saved_models\"\n",
    "\n",
    "# Load TF-IDF vectorizer & Logistic Regression model\n",
    "tfidf_vectorizer = joblib.load(f\"{model_dir}/TF-IDF_vectorizer.pkl\")\n",
    "logistic_model = joblib.load(f\"{model_dir}/TF-IDF_Logistic Regression.pkl\")\n",
    "\n",
    "# Load Word2Vec Skip-gram & MLP Classifier\n",
    "w2v_sg = Word2Vec.load(f\"{model_dir}/word2vec_skipgram.model\") \n",
    "mlp_model = joblib.load(f\"{model_dir}/Word2Vec Skip-gram_MLP Classifier.pkl\")\n",
    "\n",
    "# H√†m chuy·ªÉn c√¢u th√†nh vector v·ªõi Word2Vec Skip-gram\n",
    "def sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# H√†m ƒë·ªÉ d·ª± ƒëo√°n sentiment t·ª´ input\n",
    "def predict_sentiment(input_text):\n",
    "    # TF-IDF + Logistic Regression\n",
    "    tfidf_features = tfidf_vectorizer.transform([input_text])\n",
    "    tfidf_prediction = logistic_model.predict(tfidf_features)[0]\n",
    "\n",
    "    # Word2Vec Skip-gram + MLP Classifier\n",
    "    w2v_features = sentence_vector(input_text, w2v_sg).reshape(1, -1)\n",
    "    w2v_prediction = mlp_model.predict(w2v_features)[0]\n",
    "\n",
    "    return {\n",
    "        \"TF-IDF + Logistic Regression\": tfidf_prediction,\n",
    "        \"Word2Vec Skip-gram + MLP\": w2v_prediction\n",
    "    }\n",
    "\n",
    "# Test c√¢u ƒë·∫ßu v√†o\n",
    "# input_sentence = \"C√°i ph√≤ng ctsv r·∫•t t√≠ch c·ª±c t·ªï ch·ª©c s·ª± ki·ªán cho sinh vi√™n tham gia t·ªõ r·∫•t th√≠ch √°\"\n",
    "input_sentence = \"Gi·∫£ng d·∫°y b·ªë l√°o c∆∞·ªõp ti·ªÅn l√† gi·ªèi\"\n",
    "predictions = predict_sentiment(input_sentence)\n",
    "\n",
    "print(\"üîπ K·∫øt qu·∫£ d·ª± ƒëo√°n:\")\n",
    "sentiment_mapping = {0: \"ti√™u c·ª±c\", 1: \"b√¨nh th∆∞·ªùng\", 2: \"t√≠ch c·ª±c\"}\n",
    "mapped_predictions = {model: sentiment_mapping[pred] for model, pred in predictions.items()}\n",
    "print(mapped_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
